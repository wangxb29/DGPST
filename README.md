# DG-PST: Domain Generalizable Portrait Style Transfer
Official implementation of ["Domain Generalizable Portrait Style Transfer"](https://arxiv.org/pdf/2507.04243) (Acceped to ICCV 2025)


## Installation
Set up the python environment
``` python
conda create -n dgpst python=3.8
conda activate dgpst

pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

Download pretrained CLIP image encoder and IP-Adapter models from [here](https://huggingface.co/h94/IP-Adapter/tree/main) and put the ```models``` folder in ```./ip_adapter/```.

Download our pre-trained models from [Google drive](https://drive.google.com/drive/folders/1Nj_0tKl-Y76cOnl1BsMqlIBYNlW10TDo?usp=drive_link) and place all the files in ```./checkpoints/CelebA_default```.

Additionally, we provide another pre-trained model that demonstrates stronger stylization effects. Download it from this [link](https://drive.google.com/drive/folders/1Bn2Rthorw3noox5FCTykj7td5I5lbYr1?usp=drive_link).

The datasets used in our paper include [CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ), [AAHQ](https://github.com/onion-liu/aahq-dataset), [FS2K](https://github.com/AiArt-Gao/HIDA) and [APDrawing](https://github.com/yiranran/APDrawingGAN).

## Training

Please download [CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ) dataset. Run ```g_mask.py``` to aggregate the 19 categories and obtain a new mask folder for CelebA. Please set the ```dataroot``` path as the CelebA image path and ```dataroot2``` path to the mask path in ```./experiments/CelebA_launcher.py```, and run

``` python
python -m experiments CelebA train CelebA_train
```
To train the semantic adapter, please specify the parameter ```--training_stage``` in ```./models/DGPST_model/``` as 1.
To train the ControlNet and style adapter, please set that as 2.


If your network cannot access Hugging Face, please add 
``` python
HF_ENDPOINT=https://hf-mirror.com
```
before the training and following testing commands.

## Testing
To perform style transfer between two given images using the pretrained model in ```./checkpoints/CelebA_default/```, you can run 
``` python
CUDA_VISIBLE_DEVICES=0 --nproc_per_node=1 --master_port='29501' test.py \ 
--name CelebA_default \ 
--evaluation_metrics seed_swapping \ 
--preprocess scale_shortside \
--load_size 512 \
--model DGPST \
--input_structure_image /path/to/your/content/image \
--input_texture_image /path/to/your/style/image
```
If the result is not satisfactory, you can add a argument ```--auto_mask True``` to further refine the semantic correspondence using portrait masks generated by a Segformer [model](https://huggingface.co/jonathandinu/face-parsing). In addition, you can also use your own masks by adding
``` python
--input_structure_mask /path/to/your/content/mask \
--input_texture_mask /path/to/your/style/mask
```
after the testing command.

To perform style transfer between a folder of content images and a folder of style images, you can place both folders under the same parent directory and then specify the ```dataroot``` path in ```test_options``` of ```./experiments/CelebA_launcher.py``` as this parent folder, and change the ```dataname``` as desired to set a name for this experiment. After that, run

``` python
python -m experiments CelebA test swapping_grid
```

## Citation
If you find our work helpful for your research, please consider citing our paper.
``` python
@article{wang2025domain,
  title={Domain generalizable portrait style transfer},
  author={Wang, Xinbo and Xu, Wenju and Zhang, Qing and Zheng, Wei-Shi},
  journal={arXiv preprint arXiv:2507.04243},
  year={2025}
}
```
## Acknowledgement
We thank the great work [swapping-autoencoder](https://github.com/taesungp/swapping-autoencoder-pytorch), [IP-Adapter](https://github.com/tencent-ailab/IP-Adapter) and [dift](https://github.com/Tsingularity/dift).
